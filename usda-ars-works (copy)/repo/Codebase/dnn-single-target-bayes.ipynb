{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Global constants"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 376,
      "metadata": {},
      "outputs": [],
      "source": [
        "GRAIN_TYPE = 'WheatAdded_Type'\n",
        "#GRAIN_TYPE = 'newWheatData'\n",
        "#GRAIN_TYPE = 'CornAdded_Type'\n",
        "#GRAIN_TYPE = 'cleaned_data'\n",
        "# GRAIN_TYPE = 'Oats'\n",
        "\n",
        "# GRAIN_TYPE = 'Barley'\n",
        "# GRAIN_TYPE = 'Sorghum'\n",
        "# GRAIN_TYPE = 'Soybeans'\n",
        "# GRAIN_TYPE = 'Corn'\n",
        "\n",
        "FILENAME_BEST_MODEL = 'Best models/target_2/hybrid_models/' + GRAIN_TYPE + '_t2_kcv_dnn_mc.h5'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 377,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cNGoIGbc0kw_",
        "outputId": "279cc9c8-32fd-4f89-e56b-83a0a31081dd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2.10.0\n"
          ]
        }
      ],
      "source": [
        "#Import libraries\n",
        "import requests\n",
        "import pandas as pd\n",
        "import kerastuner as kt\n",
        "\n",
        "#Data visualization\n",
        "import seaborn as sn\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "#Data Manipulation\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Machine Learning\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model, Sequential\n",
        "from tensorflow.keras.layers import Dense, Input, Activation, BatchNormalization\n",
        "from tensorflow.keras.utils import plot_model\n",
        "from sklearn.model_selection import train_test_split, KFold\n",
        "np.random.seed(39)\n",
        "random.seed(39)\n",
        "tf.random.set_seed(39)\n",
        "print(tf.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 378,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
          ]
        }
      ],
      "source": [
        "print(tf.config.list_physical_devices('GPU'))\n",
        "# print(tf.version.VERSION)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Helper functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 379,
      "metadata": {
        "id": "nxHO_qH0Zi5J"
      },
      "outputs": [],
      "source": [
        "def calculate_r_squared(y_true, y_pred):\n",
        "   corr_matrix = np.corrcoef(y_true, y_pred)\n",
        "   corr = corr_matrix[0,1]\n",
        "   R_sq = corr**2\n",
        "   return R_sq\n",
        "\n",
        "def plot_loss_curve(history, epoch_size):\n",
        "    loss_train = history.history['loss']\n",
        "    loss_val = history.history['val_loss']\n",
        "    epochs = range(0,epoch_size)\n",
        "    \n",
        "    plt.plot(epochs, loss_train, 'g', label='Training loss')\n",
        "    plt.plot(epochs, loss_val, 'b', label='Validation loss')\n",
        "    \n",
        "    plt.title('Training and Validation loss')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Loss')\n",
        "    \n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "def plot_line(metric, title, xlabel):\n",
        "    plt.figure(figsize=(8,3))\n",
        "    plt.title(title, fontsize = 16)\n",
        "    plt.plot(metric)\n",
        "    plt.xlabel(xlabel, fontsize = 14)\n",
        "    plt.grid()\n",
        "    plt.legend(loc= \"best\")\n",
        "    plt.show()\n",
        "\n",
        "def scatter_plot(trueValues, predictions, title):\n",
        "  plt.figure(figsize=(8,3))\n",
        "  ax = plt.axes()\n",
        "  maxVal = max( max(trueValues), max(predictions) )\n",
        "\n",
        "  ax.scatter(x=predictions, y=trueValues)\n",
        "  ax.plot([0, 1, maxVal], [0, 1, maxVal], label=\"Ideal fit\")\n",
        "  print('Maxval here is: ', maxVal)\n",
        "  plt.title(title, fontsize = 16)\n",
        "  plt.xlabel(\"Predictions\", fontsize = 14)\n",
        "  plt.ylabel(\"Real\", fontsize = 14)\n",
        "  plt.grid()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 380,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        },
        "id": "s3pvA5g-zdgv",
        "outputId": "7a7208f1-6b68-4eba-ad1d-9108d0df66ac"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "From USDA:  ../Datasets/processed/WheatAdded_Type.csv\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>Variety</th>\n",
              "      <th>Freq</th>\n",
              "      <th>d(cm)</th>\n",
              "      <th>M%</th>\n",
              "      <th>Density</th>\n",
              "      <th>Attn</th>\n",
              "      <th>Phase</th>\n",
              "      <th>Phase_Corr</th>\n",
              "      <th>Permittivity_real</th>\n",
              "      <th>Permittivity_imaginary</th>\n",
              "      <th>Type</th>\n",
              "      <th>Phase/Attn</th>\n",
              "      <th>Freq*d(cm)</th>\n",
              "      <th>Freq*Attn</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>KANSAS</td>\n",
              "      <td>7.0</td>\n",
              "      <td>8.9</td>\n",
              "      <td>11.3</td>\n",
              "      <td>0.7356</td>\n",
              "      <td>8.8258</td>\n",
              "      <td>-55.973</td>\n",
              "      <td>-415.973</td>\n",
              "      <td>2.416</td>\n",
              "      <td>0.243</td>\n",
              "      <td>15.855506</td>\n",
              "      <td>-6.341975</td>\n",
              "      <td>62.3</td>\n",
              "      <td>61.7806</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>KANSAS</td>\n",
              "      <td>8.0</td>\n",
              "      <td>8.9</td>\n",
              "      <td>11.3</td>\n",
              "      <td>0.7356</td>\n",
              "      <td>10.2572</td>\n",
              "      <td>-114.289</td>\n",
              "      <td>-474.289</td>\n",
              "      <td>2.412</td>\n",
              "      <td>0.246</td>\n",
              "      <td>15.855506</td>\n",
              "      <td>-11.142320</td>\n",
              "      <td>71.2</td>\n",
              "      <td>82.0576</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>KANSAS</td>\n",
              "      <td>9.0</td>\n",
              "      <td>8.9</td>\n",
              "      <td>11.3</td>\n",
              "      <td>0.7356</td>\n",
              "      <td>11.5679</td>\n",
              "      <td>-168.171</td>\n",
              "      <td>-528.171</td>\n",
              "      <td>2.395</td>\n",
              "      <td>0.246</td>\n",
              "      <td>15.855506</td>\n",
              "      <td>-14.537729</td>\n",
              "      <td>80.1</td>\n",
              "      <td>104.1111</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>KANSAS</td>\n",
              "      <td>10.0</td>\n",
              "      <td>8.9</td>\n",
              "      <td>11.3</td>\n",
              "      <td>0.7356</td>\n",
              "      <td>12.8795</td>\n",
              "      <td>134.849</td>\n",
              "      <td>-585.151</td>\n",
              "      <td>2.390</td>\n",
              "      <td>0.246</td>\n",
              "      <td>15.855506</td>\n",
              "      <td>10.470049</td>\n",
              "      <td>89.0</td>\n",
              "      <td>128.7950</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>KANSAS</td>\n",
              "      <td>11.0</td>\n",
              "      <td>8.9</td>\n",
              "      <td>11.3</td>\n",
              "      <td>0.7356</td>\n",
              "      <td>13.7649</td>\n",
              "      <td>83.502</td>\n",
              "      <td>-636.498</td>\n",
              "      <td>2.371</td>\n",
              "      <td>0.238</td>\n",
              "      <td>15.855506</td>\n",
              "      <td>6.066299</td>\n",
              "      <td>97.9</td>\n",
              "      <td>151.4139</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Unnamed: 0 Variety  Freq  d(cm)    M%  Density     Attn    Phase  \\\n",
              "0           0  KANSAS   7.0    8.9  11.3   0.7356   8.8258  -55.973   \n",
              "1           1  KANSAS   8.0    8.9  11.3   0.7356  10.2572 -114.289   \n",
              "2           2  KANSAS   9.0    8.9  11.3   0.7356  11.5679 -168.171   \n",
              "3           3  KANSAS  10.0    8.9  11.3   0.7356  12.8795  134.849   \n",
              "4           4  KANSAS  11.0    8.9  11.3   0.7356  13.7649   83.502   \n",
              "\n",
              "   Phase_Corr  Permittivity_real  Permittivity_imaginary       Type  \\\n",
              "0    -415.973              2.416                   0.243  15.855506   \n",
              "1    -474.289              2.412                   0.246  15.855506   \n",
              "2    -528.171              2.395                   0.246  15.855506   \n",
              "3    -585.151              2.390                   0.246  15.855506   \n",
              "4    -636.498              2.371                   0.238  15.855506   \n",
              "\n",
              "   Phase/Attn  Freq*d(cm)  Freq*Attn  \n",
              "0   -6.341975        62.3    61.7806  \n",
              "1  -11.142320        71.2    82.0576  \n",
              "2  -14.537729        80.1   104.1111  \n",
              "3   10.470049        89.0   128.7950  \n",
              "4    6.066299        97.9   151.4139  "
            ]
          },
          "execution_count": 380,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#url dataset\n",
        "URL = \"../Datasets/processed/\" + GRAIN_TYPE + \".csv\"\n",
        "\n",
        "#read in excel format\n",
        "df = pd.read_csv(URL)\n",
        "#df = df[df['Variety'] == 'SOUTH DAKOTA']\n",
        "#df = df[(df['Density'] >= 0.72) & (df['Density'] <= 0.88)]\n",
        "\n",
        "print(\"From USDA: \", URL)\n",
        "\n",
        "\n",
        "df.head()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "_LUzjHHV2stm"
      },
      "source": [
        "# 2. Overview of data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 381,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 364
        },
        "id": "Xohz7dGh2sXH",
        "outputId": "7d018cd8-018a-45d3-b1b7-ba9fc14aa5e3"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>Freq</th>\n",
              "      <th>d(cm)</th>\n",
              "      <th>M%</th>\n",
              "      <th>Density</th>\n",
              "      <th>Attn</th>\n",
              "      <th>Phase</th>\n",
              "      <th>Phase_Corr</th>\n",
              "      <th>Permittivity_real</th>\n",
              "      <th>Permittivity_imaginary</th>\n",
              "      <th>Type</th>\n",
              "      <th>Phase/Attn</th>\n",
              "      <th>Freq*d(cm)</th>\n",
              "      <th>Freq*Attn</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>806.000000</td>\n",
              "      <td>806.000000</td>\n",
              "      <td>806.000000</td>\n",
              "      <td>806.000000</td>\n",
              "      <td>806.000000</td>\n",
              "      <td>806.000000</td>\n",
              "      <td>806.000000</td>\n",
              "      <td>806.000000</td>\n",
              "      <td>806.000000</td>\n",
              "      <td>806.000000</td>\n",
              "      <td>806.000000</td>\n",
              "      <td>806.000000</td>\n",
              "      <td>806.000000</td>\n",
              "      <td>806.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>402.500000</td>\n",
              "      <td>10.811414</td>\n",
              "      <td>7.088834</td>\n",
              "      <td>16.189541</td>\n",
              "      <td>0.796298</td>\n",
              "      <td>18.410033</td>\n",
              "      <td>-4.604663</td>\n",
              "      <td>-633.488065</td>\n",
              "      <td>2.912112</td>\n",
              "      <td>0.499187</td>\n",
              "      <td>16.189541</td>\n",
              "      <td>-0.377074</td>\n",
              "      <td>77.159677</td>\n",
              "      <td>215.799030</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>232.816451</td>\n",
              "      <td>3.530055</td>\n",
              "      <td>1.554604</td>\n",
              "      <td>3.794772</td>\n",
              "      <td>0.067384</td>\n",
              "      <td>5.946835</td>\n",
              "      <td>101.951444</td>\n",
              "      <td>219.510760</td>\n",
              "      <td>0.305758</td>\n",
              "      <td>0.186739</td>\n",
              "      <td>0.629743</td>\n",
              "      <td>6.071761</td>\n",
              "      <td>32.552200</td>\n",
              "      <td>124.108325</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>5.000000</td>\n",
              "      <td>4.400000</td>\n",
              "      <td>10.260000</td>\n",
              "      <td>0.625400</td>\n",
              "      <td>8.002300</td>\n",
              "      <td>-179.335000</td>\n",
              "      <td>-1274.435000</td>\n",
              "      <td>2.340000</td>\n",
              "      <td>0.220000</td>\n",
              "      <td>15.352809</td>\n",
              "      <td>-17.418676</td>\n",
              "      <td>22.000000</td>\n",
              "      <td>40.011500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>201.250000</td>\n",
              "      <td>8.000000</td>\n",
              "      <td>6.500000</td>\n",
              "      <td>13.680000</td>\n",
              "      <td>0.745400</td>\n",
              "      <td>13.524700</td>\n",
              "      <td>-88.842000</td>\n",
              "      <td>-793.405750</td>\n",
              "      <td>2.688500</td>\n",
              "      <td>0.337000</td>\n",
              "      <td>15.855506</td>\n",
              "      <td>-5.077754</td>\n",
              "      <td>52.800000</td>\n",
              "      <td>107.817375</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>402.500000</td>\n",
              "      <td>11.000000</td>\n",
              "      <td>7.700000</td>\n",
              "      <td>16.225000</td>\n",
              "      <td>0.801300</td>\n",
              "      <td>18.131600</td>\n",
              "      <td>-9.838500</td>\n",
              "      <td>-602.380500</td>\n",
              "      <td>2.861500</td>\n",
              "      <td>0.470500</td>\n",
              "      <td>16.400366</td>\n",
              "      <td>-0.589378</td>\n",
              "      <td>71.200000</td>\n",
              "      <td>195.600450</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>603.750000</td>\n",
              "      <td>13.000000</td>\n",
              "      <td>7.700000</td>\n",
              "      <td>18.810000</td>\n",
              "      <td>0.842000</td>\n",
              "      <td>23.098000</td>\n",
              "      <td>80.957250</td>\n",
              "      <td>-456.055750</td>\n",
              "      <td>3.109750</td>\n",
              "      <td>0.639000</td>\n",
              "      <td>16.401988</td>\n",
              "      <td>4.300734</td>\n",
              "      <td>100.100000</td>\n",
              "      <td>310.863000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>805.000000</td>\n",
              "      <td>18.000000</td>\n",
              "      <td>8.900000</td>\n",
              "      <td>24.410000</td>\n",
              "      <td>0.927800</td>\n",
              "      <td>29.897000</td>\n",
              "      <td>179.048000</td>\n",
              "      <td>-235.044000</td>\n",
              "      <td>4.038000</td>\n",
              "      <td>0.987000</td>\n",
              "      <td>17.344167</td>\n",
              "      <td>14.827701</td>\n",
              "      <td>160.200000</td>\n",
              "      <td>538.146000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       Unnamed: 0        Freq       d(cm)          M%     Density        Attn  \\\n",
              "count  806.000000  806.000000  806.000000  806.000000  806.000000  806.000000   \n",
              "mean   402.500000   10.811414    7.088834   16.189541    0.796298   18.410033   \n",
              "std    232.816451    3.530055    1.554604    3.794772    0.067384    5.946835   \n",
              "min      0.000000    5.000000    4.400000   10.260000    0.625400    8.002300   \n",
              "25%    201.250000    8.000000    6.500000   13.680000    0.745400   13.524700   \n",
              "50%    402.500000   11.000000    7.700000   16.225000    0.801300   18.131600   \n",
              "75%    603.750000   13.000000    7.700000   18.810000    0.842000   23.098000   \n",
              "max    805.000000   18.000000    8.900000   24.410000    0.927800   29.897000   \n",
              "\n",
              "            Phase   Phase_Corr  Permittivity_real  Permittivity_imaginary  \\\n",
              "count  806.000000   806.000000         806.000000              806.000000   \n",
              "mean    -4.604663  -633.488065           2.912112                0.499187   \n",
              "std    101.951444   219.510760           0.305758                0.186739   \n",
              "min   -179.335000 -1274.435000           2.340000                0.220000   \n",
              "25%    -88.842000  -793.405750           2.688500                0.337000   \n",
              "50%     -9.838500  -602.380500           2.861500                0.470500   \n",
              "75%     80.957250  -456.055750           3.109750                0.639000   \n",
              "max    179.048000  -235.044000           4.038000                0.987000   \n",
              "\n",
              "             Type  Phase/Attn  Freq*d(cm)   Freq*Attn  \n",
              "count  806.000000  806.000000  806.000000  806.000000  \n",
              "mean    16.189541   -0.377074   77.159677  215.799030  \n",
              "std      0.629743    6.071761   32.552200  124.108325  \n",
              "min     15.352809  -17.418676   22.000000   40.011500  \n",
              "25%     15.855506   -5.077754   52.800000  107.817375  \n",
              "50%     16.400366   -0.589378   71.200000  195.600450  \n",
              "75%     16.401988    4.300734  100.100000  310.863000  \n",
              "max     17.344167   14.827701  160.200000  538.146000  "
            ]
          },
          "execution_count": 381,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Data summary\n",
        "df.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 382,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SYmFqsYQyGnM",
        "outputId": "54445a7f-a2c8-452a-9651-42dbbe682d2b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(806, 15)"
            ]
          },
          "execution_count": 382,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Dimension of the dataset\n",
        "df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 383,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fep-GIv4yUuf",
        "outputId": "c46072fa-aa7f-4549-9a1d-4c5b05d11112"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Unnamed: 0                0\n",
              "Variety                   0\n",
              "Freq                      0\n",
              "d(cm)                     0\n",
              "M%                        0\n",
              "Density                   0\n",
              "Attn                      0\n",
              "Phase                     0\n",
              "Phase_Corr                0\n",
              "Permittivity_real         0\n",
              "Permittivity_imaginary    0\n",
              "Type                      0\n",
              "Phase/Attn                0\n",
              "Freq*d(cm)                0\n",
              "Freq*Attn                 0\n",
              "dtype: int64"
            ]
          },
          "execution_count": 383,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Check info about missing values in dataframe\n",
        "df.isnull().sum()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "OI_TKP9VymuK"
      },
      "source": [
        "# Exploratory Data Analysis\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Oz1g9T3FzhF0"
      },
      "source": [
        "# Data preparation\n",
        "\n",
        "\n",
        "1.   Convert dataframe to numpy array for flexibility.\n",
        "2. Split our data into training and testing datasets and store the target values in different variables.\n",
        "3.   Normalize the features by applying some operations in the data sets.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 384,
      "metadata": {
        "id": "T0juhagf1M2I"
      },
      "outputs": [],
      "source": [
        "# Convert to numpy array\n",
        "df_features = df[['Freq', \n",
        "                    'd(cm)', \n",
        "                   # 'Attn', \n",
        "                    'Phase_Corr', \n",
        "                    'Permittivity_real', \n",
        "                    'Permittivity_imaginary',\n",
        "                    'Type',\n",
        "                    ]]\n",
        "\n",
        "df_targets = df[['Density']]\n",
        "# df_targets = df[['Density', 'M%']]\n",
        "\n",
        "dataset_x = df_features.to_numpy()\n",
        "dataset_y = df_targets.to_numpy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Splitting dataset to test and train+validate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 385,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "#Normalizing the data set\n",
        "scaler_input = MinMaxScaler()\n",
        "scaler_output = MinMaxScaler()\n",
        "\n",
        "# Normalize the entire dataset (input features)\n",
        "dataset_x_norm = scaler_input.fit_transform(dataset_x)  # Use transform, NOT fit_transform\n",
        "\n",
        "# Normalize the entire dataset (output targets)\n",
        "dataset_y_norm = scaler_output.fit_transform(dataset_y)  # Use transform, NOT fit_transform\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Normalize datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 386,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Perform train-test split on RAW DATA\n",
        "X_train, X_test, y_train, y_test = train_test_split(dataset_x_norm, dataset_y_norm, \n",
        "                                                    test_size=0.15\n",
        "                                                    ,random_state=42\n",
        "                                                    )\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.15, random_state=42)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "JgKfjwMP0Tzn"
      },
      "source": [
        "# K-cross Validation\n",
        "* Input features: 7\n",
        "* Output targets: 2\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Defining model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 387,
      "metadata": {
        "id": "l31WJZ7Z0ONb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model to work.\n",
            "Model: \"sequential_10898\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_67986 (Dense)         (None, 109)               763       \n",
            "                                                                 \n",
            " dense_67987 (Dense)         (None, 109)               11990     \n",
            "                                                                 \n",
            " dense_67988 (Dense)         (None, 109)               11990     \n",
            "                                                                 \n",
            " dense_67989 (Dense)         (None, 1)                 110       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 24,853\n",
            "Trainable params: 24,853\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "from keras import layers, Sequential, regularizers\n",
        "\n",
        "# Define the model-building function\n",
        "def my_model():\n",
        "  my_model = Sequential([\n",
        "    \n",
        "    layers.Dense(109, input_shape=(6,), activation='relu',),\n",
        "    layers.Dense(109, activation='relu', ),\n",
        "    layers.Dense(109, activation='relu',),\n",
        "    layers.Dense(1, activation='linear')  # Output layer with 2 neurons for the two regression targets\n",
        "  ])\n",
        "\n",
        "  opt = tf.keras.optimizers.Adam(learning_rate=0.00091) # 0.0006 \n",
        "  my_model.compile(\n",
        "      optimizer = opt,\n",
        "      loss = 'mse',\n",
        "      metrics = ['accuracy']\n",
        "  )\n",
        "\n",
        "  return my_model\n",
        "\n",
        "plot_model(my_model(), show_shapes=True, show_layer_names=True)\n",
        "my_model().summary()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Running model with KCV"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 388,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "khCKKB74hFVT",
        "outputId": "37e79cdf-4183-4559-f560-fceb2fc0c630"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/10700 [00:00<?, ?trial/s, best loss=?]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "100%|██████████| 10700/10700 [17:22:54<00:00,  5.85s/trial, best loss: 6.368294998537749e-05]  \n",
            "Best hyperparameters: {'activation': 'relu', 'epochs': 200, 'learning_rate': 0.0025127745346078647, 'num_layers': 7, 'units_per_layer': 300}\n",
            "Epoch 1/200\n",
            "22/22 [==============================] - 0s 840us/step - loss: 0.1574\n",
            "Epoch 2/200\n",
            "22/22 [==============================] - 0s 825us/step - loss: 0.0327\n",
            "Epoch 3/200\n",
            "22/22 [==============================] - 0s 836us/step - loss: 0.0133\n",
            "Epoch 4/200\n",
            "22/22 [==============================] - 0s 845us/step - loss: 0.0094\n",
            "Epoch 5/200\n",
            "22/22 [==============================] - 0s 837us/step - loss: 0.0083\n",
            "Epoch 6/200\n",
            "22/22 [==============================] - 0s 791us/step - loss: 0.0067\n",
            "Epoch 7/200\n",
            "22/22 [==============================] - 0s 841us/step - loss: 0.0068\n",
            "Epoch 8/200\n",
            "22/22 [==============================] - 0s 870us/step - loss: 0.0058\n",
            "Epoch 9/200\n",
            "22/22 [==============================] - 0s 855us/step - loss: 0.0065\n",
            "Epoch 10/200\n",
            "22/22 [==============================] - 0s 877us/step - loss: 0.0048\n",
            "Epoch 11/200\n",
            "22/22 [==============================] - 0s 898us/step - loss: 0.0038\n",
            "Epoch 12/200\n",
            "22/22 [==============================] - 0s 937us/step - loss: 0.0037\n",
            "Epoch 13/200\n",
            "22/22 [==============================] - 0s 880us/step - loss: 0.0031\n",
            "Epoch 14/200\n",
            "22/22 [==============================] - 0s 931us/step - loss: 0.0025\n",
            "Epoch 15/200\n",
            "22/22 [==============================] - 0s 874us/step - loss: 0.0023\n",
            "Epoch 16/200\n",
            "22/22 [==============================] - 0s 874us/step - loss: 0.0035\n",
            "Epoch 17/200\n",
            "22/22 [==============================] - 0s 845us/step - loss: 0.0021\n",
            "Epoch 18/200\n",
            "22/22 [==============================] - 0s 848us/step - loss: 0.0018\n",
            "Epoch 19/200\n",
            "22/22 [==============================] - 0s 850us/step - loss: 0.0018\n",
            "Epoch 20/200\n",
            "22/22 [==============================] - 0s 843us/step - loss: 0.0023\n",
            "Epoch 21/200\n",
            "22/22 [==============================] - 0s 832us/step - loss: 0.0022\n",
            "Epoch 22/200\n",
            "22/22 [==============================] - 0s 846us/step - loss: 0.0030\n",
            "Epoch 23/200\n",
            "22/22 [==============================] - 0s 842us/step - loss: 0.0017\n",
            "Epoch 24/200\n",
            "22/22 [==============================] - 0s 833us/step - loss: 0.0011\n",
            "Epoch 25/200\n",
            "22/22 [==============================] - 0s 842us/step - loss: 0.0012\n",
            "Epoch 26/200\n",
            "22/22 [==============================] - 0s 837us/step - loss: 0.0015\n",
            "Epoch 27/200\n",
            "22/22 [==============================] - 0s 809us/step - loss: 0.0022\n",
            "Epoch 28/200\n",
            "22/22 [==============================] - 0s 840us/step - loss: 0.0016\n",
            "Epoch 29/200\n",
            "22/22 [==============================] - 0s 850us/step - loss: 0.0016\n",
            "Epoch 30/200\n",
            "22/22 [==============================] - 0s 842us/step - loss: 0.0014\n",
            "Epoch 31/200\n",
            "22/22 [==============================] - 0s 843us/step - loss: 0.0013\n",
            "Epoch 32/200\n",
            "22/22 [==============================] - 0s 836us/step - loss: 8.9546e-04\n",
            "Epoch 33/200\n",
            "22/22 [==============================] - 0s 856us/step - loss: 8.1081e-04\n",
            "Epoch 34/200\n",
            "22/22 [==============================] - 0s 874us/step - loss: 8.1491e-04\n",
            "Epoch 35/200\n",
            "22/22 [==============================] - 0s 859us/step - loss: 0.0012\n",
            "Epoch 36/200\n",
            "22/22 [==============================] - 0s 816us/step - loss: 0.0014\n",
            "Epoch 37/200\n",
            "22/22 [==============================] - 0s 850us/step - loss: 0.0011\n",
            "Epoch 38/200\n",
            "22/22 [==============================] - 0s 853us/step - loss: 0.0011\n",
            "Epoch 39/200\n",
            "22/22 [==============================] - 0s 863us/step - loss: 8.6902e-04\n",
            "Epoch 40/200\n",
            "22/22 [==============================] - 0s 875us/step - loss: 9.4602e-04\n",
            "Epoch 41/200\n",
            "22/22 [==============================] - 0s 859us/step - loss: 0.0017\n",
            "Epoch 42/200\n",
            "22/22 [==============================] - 0s 856us/step - loss: 0.0013\n",
            "Epoch 43/200\n",
            "22/22 [==============================] - 0s 853us/step - loss: 8.7360e-04\n",
            "Epoch 44/200\n",
            "22/22 [==============================] - 0s 860us/step - loss: 9.1528e-04\n",
            "Epoch 45/200\n",
            "22/22 [==============================] - 0s 868us/step - loss: 0.0014\n",
            "Epoch 46/200\n",
            "22/22 [==============================] - 0s 867us/step - loss: 0.0018\n",
            "Epoch 47/200\n",
            "22/22 [==============================] - 0s 870us/step - loss: 0.0014\n",
            "Epoch 48/200\n",
            "22/22 [==============================] - 0s 879us/step - loss: 0.0010\n",
            "Epoch 49/200\n",
            "22/22 [==============================] - 0s 845us/step - loss: 0.0013\n",
            "Epoch 50/200\n",
            "22/22 [==============================] - 0s 861us/step - loss: 8.4685e-04\n",
            "Epoch 51/200\n",
            "22/22 [==============================] - 0s 853us/step - loss: 8.8236e-04\n",
            "Epoch 52/200\n",
            "22/22 [==============================] - 0s 875us/step - loss: 0.0014\n",
            "Epoch 53/200\n",
            "22/22 [==============================] - 0s 962us/step - loss: 0.0020\n",
            "Epoch 54/200\n",
            "22/22 [==============================] - 0s 871us/step - loss: 9.7183e-04\n",
            "Epoch 55/200\n",
            "22/22 [==============================] - 0s 879us/step - loss: 8.3511e-04\n",
            "Epoch 56/200\n",
            "22/22 [==============================] - 0s 914us/step - loss: 0.0019\n",
            "Epoch 57/200\n",
            "22/22 [==============================] - 0s 882us/step - loss: 0.0014\n",
            "Epoch 58/200\n",
            "22/22 [==============================] - 0s 860us/step - loss: 0.0015\n",
            "Epoch 59/200\n",
            "22/22 [==============================] - 0s 872us/step - loss: 0.0029\n",
            "Epoch 60/200\n",
            "22/22 [==============================] - 0s 887us/step - loss: 0.0020\n",
            "Epoch 61/200\n",
            "22/22 [==============================] - 0s 846us/step - loss: 9.3487e-04\n",
            "Epoch 62/200\n",
            "22/22 [==============================] - 0s 930us/step - loss: 8.5444e-04\n",
            "Epoch 63/200\n",
            "22/22 [==============================] - 0s 863us/step - loss: 4.6488e-04\n",
            "Epoch 64/200\n",
            "22/22 [==============================] - 0s 863us/step - loss: 6.7490e-04\n",
            "Epoch 65/200\n",
            "22/22 [==============================] - 0s 873us/step - loss: 6.6745e-04\n",
            "Epoch 66/200\n",
            "22/22 [==============================] - 0s 853us/step - loss: 7.6858e-04\n",
            "Epoch 67/200\n",
            "22/22 [==============================] - 0s 860us/step - loss: 5.6497e-04\n",
            "Epoch 68/200\n",
            "22/22 [==============================] - 0s 850us/step - loss: 4.4712e-04\n",
            "Epoch 69/200\n",
            "22/22 [==============================] - 0s 850us/step - loss: 7.7646e-04\n",
            "Epoch 70/200\n",
            "22/22 [==============================] - 0s 842us/step - loss: 5.3140e-04\n",
            "Epoch 71/200\n",
            "22/22 [==============================] - 0s 858us/step - loss: 3.5861e-04\n",
            "Epoch 72/200\n",
            "22/22 [==============================] - 0s 855us/step - loss: 5.1006e-04\n",
            "Epoch 73/200\n",
            "22/22 [==============================] - 0s 848us/step - loss: 3.2548e-04\n",
            "Epoch 74/200\n",
            "22/22 [==============================] - 0s 843us/step - loss: 3.1333e-04\n",
            "Epoch 75/200\n",
            "22/22 [==============================] - 0s 855us/step - loss: 4.1637e-04\n",
            "Epoch 76/200\n",
            "22/22 [==============================] - 0s 856us/step - loss: 6.0005e-04\n",
            "Epoch 77/200\n",
            "22/22 [==============================] - 0s 845us/step - loss: 5.5991e-04\n",
            "Epoch 78/200\n",
            "22/22 [==============================] - 0s 859us/step - loss: 0.0021\n",
            "Epoch 79/200\n",
            "22/22 [==============================] - 0s 849us/step - loss: 8.4666e-04\n",
            "Epoch 80/200\n",
            "22/22 [==============================] - 0s 860us/step - loss: 5.4412e-04\n",
            "Epoch 81/200\n",
            "22/22 [==============================] - 0s 849us/step - loss: 6.2864e-04\n",
            "Epoch 82/200\n",
            "22/22 [==============================] - 0s 985us/step - loss: 4.9672e-04\n",
            "Epoch 83/200\n",
            "22/22 [==============================] - 0s 924us/step - loss: 0.0014\n",
            "Epoch 84/200\n",
            "22/22 [==============================] - 0s 867us/step - loss: 7.5393e-04\n",
            "Epoch 85/200\n",
            "22/22 [==============================] - 0s 870us/step - loss: 9.2435e-04\n",
            "Epoch 86/200\n",
            "22/22 [==============================] - 0s 862us/step - loss: 8.4445e-04\n",
            "Epoch 87/200\n",
            "22/22 [==============================] - 0s 858us/step - loss: 4.3634e-04\n",
            "Epoch 88/200\n",
            "22/22 [==============================] - 0s 837us/step - loss: 6.6256e-04\n",
            "Epoch 89/200\n",
            "22/22 [==============================] - 0s 837us/step - loss: 8.1098e-04\n",
            "Epoch 90/200\n",
            "22/22 [==============================] - 0s 927us/step - loss: 4.5032e-04\n",
            "Epoch 91/200\n",
            "22/22 [==============================] - 0s 886us/step - loss: 8.7605e-04\n",
            "Epoch 92/200\n",
            "22/22 [==============================] - 0s 886us/step - loss: 5.1470e-04\n",
            "Epoch 93/200\n",
            "22/22 [==============================] - 0s 857us/step - loss: 4.9085e-04\n",
            "Epoch 94/200\n",
            "22/22 [==============================] - 0s 847us/step - loss: 6.4355e-04\n",
            "Epoch 95/200\n",
            "22/22 [==============================] - 0s 850us/step - loss: 0.0010\n",
            "Epoch 96/200\n",
            "22/22 [==============================] - 0s 846us/step - loss: 5.4084e-04\n",
            "Epoch 97/200\n",
            "22/22 [==============================] - 0s 847us/step - loss: 4.3575e-04\n",
            "Epoch 98/200\n",
            "22/22 [==============================] - 0s 841us/step - loss: 9.9845e-04\n",
            "Epoch 99/200\n",
            "22/22 [==============================] - 0s 849us/step - loss: 6.2440e-04\n",
            "Epoch 100/200\n",
            "22/22 [==============================] - 0s 870us/step - loss: 4.7313e-04\n",
            "Epoch 101/200\n",
            "22/22 [==============================] - 0s 855us/step - loss: 2.8464e-04\n",
            "Epoch 102/200\n",
            "22/22 [==============================] - 0s 849us/step - loss: 4.4455e-04\n",
            "Epoch 103/200\n",
            "22/22 [==============================] - 0s 845us/step - loss: 5.1675e-04\n",
            "Epoch 104/200\n",
            "22/22 [==============================] - 0s 838us/step - loss: 4.1293e-04\n",
            "Epoch 105/200\n",
            "22/22 [==============================] - 0s 850us/step - loss: 2.9134e-04\n",
            "Epoch 106/200\n",
            "22/22 [==============================] - 0s 853us/step - loss: 3.3459e-04\n",
            "Epoch 107/200\n",
            "22/22 [==============================] - 0s 892us/step - loss: 3.3567e-04\n",
            "Epoch 108/200\n",
            "22/22 [==============================] - 0s 871us/step - loss: 2.9841e-04\n",
            "Epoch 109/200\n",
            "22/22 [==============================] - 0s 899us/step - loss: 3.9749e-04\n",
            "Epoch 110/200\n",
            "22/22 [==============================] - 0s 826us/step - loss: 3.5845e-04\n",
            "Epoch 111/200\n",
            "22/22 [==============================] - 0s 884us/step - loss: 4.7553e-04\n",
            "Epoch 112/200\n",
            "22/22 [==============================] - 0s 897us/step - loss: 4.9948e-04\n",
            "Epoch 113/200\n",
            "22/22 [==============================] - 0s 870us/step - loss: 9.6368e-04\n",
            "Epoch 114/200\n",
            "22/22 [==============================] - 0s 864us/step - loss: 6.7362e-04\n",
            "Epoch 115/200\n",
            "22/22 [==============================] - 0s 852us/step - loss: 3.8344e-04\n",
            "Epoch 116/200\n",
            "22/22 [==============================] - 0s 861us/step - loss: 4.8797e-04\n",
            "Epoch 117/200\n",
            "22/22 [==============================] - 0s 855us/step - loss: 4.4795e-04\n",
            "Epoch 118/200\n",
            "22/22 [==============================] - 0s 864us/step - loss: 2.4551e-04\n",
            "Epoch 119/200\n",
            "22/22 [==============================] - 0s 843us/step - loss: 4.2011e-04\n",
            "Epoch 120/200\n",
            "22/22 [==============================] - 0s 870us/step - loss: 4.7716e-04\n",
            "Epoch 121/200\n",
            "22/22 [==============================] - 0s 869us/step - loss: 4.3471e-04\n",
            "Epoch 122/200\n",
            "22/22 [==============================] - 0s 843us/step - loss: 3.3636e-04\n",
            "Epoch 123/200\n",
            "22/22 [==============================] - 0s 848us/step - loss: 4.5505e-04\n",
            "Epoch 124/200\n",
            "22/22 [==============================] - 0s 846us/step - loss: 5.4784e-04\n",
            "Epoch 125/200\n",
            "22/22 [==============================] - 0s 845us/step - loss: 6.2490e-04\n",
            "Epoch 126/200\n",
            "22/22 [==============================] - 0s 846us/step - loss: 3.3066e-04\n",
            "Epoch 127/200\n",
            "22/22 [==============================] - 0s 857us/step - loss: 2.3409e-04\n",
            "Epoch 128/200\n",
            "22/22 [==============================] - 0s 967us/step - loss: 5.0337e-04\n",
            "Epoch 129/200\n",
            "22/22 [==============================] - 0s 868us/step - loss: 0.0012\n",
            "Epoch 130/200\n",
            "22/22 [==============================] - 0s 825us/step - loss: 0.0011\n",
            "Epoch 131/200\n",
            "22/22 [==============================] - 0s 837us/step - loss: 0.0019\n",
            "Epoch 132/200\n",
            "22/22 [==============================] - 0s 814us/step - loss: 0.0054\n",
            "Epoch 133/200\n",
            "22/22 [==============================] - 0s 834us/step - loss: 0.0019\n",
            "Epoch 134/200\n",
            "22/22 [==============================] - 0s 851us/step - loss: 0.0021\n",
            "Epoch 135/200\n",
            "22/22 [==============================] - 0s 837us/step - loss: 8.9972e-04\n",
            "Epoch 136/200\n",
            "22/22 [==============================] - 0s 847us/step - loss: 7.1735e-04\n",
            "Epoch 137/200\n",
            "22/22 [==============================] - 0s 861us/step - loss: 5.2761e-04\n",
            "Epoch 138/200\n",
            "22/22 [==============================] - 0s 826us/step - loss: 3.2821e-04\n",
            "Epoch 139/200\n",
            "22/22 [==============================] - 0s 778us/step - loss: 4.5595e-04\n",
            "Epoch 140/200\n",
            "22/22 [==============================] - 0s 810us/step - loss: 4.4136e-04\n",
            "Epoch 141/200\n",
            "22/22 [==============================] - 0s 844us/step - loss: 4.0975e-04\n",
            "Epoch 142/200\n",
            "22/22 [==============================] - 0s 828us/step - loss: 2.7687e-04\n",
            "Epoch 143/200\n",
            "22/22 [==============================] - 0s 818us/step - loss: 2.5152e-04\n",
            "Epoch 144/200\n",
            "22/22 [==============================] - 0s 830us/step - loss: 3.2164e-04\n",
            "Epoch 145/200\n",
            "22/22 [==============================] - 0s 948us/step - loss: 2.8449e-04\n",
            "Epoch 146/200\n",
            "22/22 [==============================] - 0s 930us/step - loss: 2.1203e-04\n",
            "Epoch 147/200\n",
            "22/22 [==============================] - 0s 829us/step - loss: 1.7777e-04\n",
            "Epoch 148/200\n",
            "22/22 [==============================] - 0s 846us/step - loss: 1.9149e-04\n",
            "Epoch 149/200\n",
            "22/22 [==============================] - 0s 842us/step - loss: 2.5911e-04\n",
            "Epoch 150/200\n",
            "22/22 [==============================] - 0s 822us/step - loss: 5.7166e-04\n",
            "Epoch 151/200\n",
            "22/22 [==============================] - 0s 845us/step - loss: 0.0013\n",
            "Epoch 152/200\n",
            "22/22 [==============================] - 0s 841us/step - loss: 8.6472e-04\n",
            "Epoch 153/200\n",
            "22/22 [==============================] - 0s 837us/step - loss: 6.5529e-04\n",
            "Epoch 154/200\n",
            "22/22 [==============================] - 0s 845us/step - loss: 3.7082e-04\n",
            "Epoch 155/200\n",
            "22/22 [==============================] - 0s 841us/step - loss: 2.7506e-04\n",
            "Epoch 156/200\n",
            "22/22 [==============================] - 0s 845us/step - loss: 3.5886e-04\n",
            "Epoch 157/200\n",
            "22/22 [==============================] - 0s 839us/step - loss: 2.7100e-04\n",
            "Epoch 158/200\n",
            "22/22 [==============================] - 0s 842us/step - loss: 2.0481e-04\n",
            "Epoch 159/200\n",
            "22/22 [==============================] - 0s 883us/step - loss: 2.5851e-04\n",
            "Epoch 160/200\n",
            "22/22 [==============================] - 0s 916us/step - loss: 2.4664e-04\n",
            "Epoch 161/200\n",
            "22/22 [==============================] - 0s 909us/step - loss: 5.5701e-04\n",
            "Epoch 162/200\n",
            "22/22 [==============================] - 0s 862us/step - loss: 4.1502e-04\n",
            "Epoch 163/200\n",
            "22/22 [==============================] - 0s 911us/step - loss: 3.8120e-04\n",
            "Epoch 164/200\n",
            "22/22 [==============================] - 0s 883us/step - loss: 2.9453e-04\n",
            "Epoch 165/200\n",
            "22/22 [==============================] - 0s 842us/step - loss: 7.4594e-04\n",
            "Epoch 166/200\n",
            "22/22 [==============================] - 0s 844us/step - loss: 4.4923e-04\n",
            "Epoch 167/200\n",
            "22/22 [==============================] - 0s 930us/step - loss: 0.0017\n",
            "Epoch 168/200\n",
            "22/22 [==============================] - 0s 857us/step - loss: 8.8599e-04\n",
            "Epoch 169/200\n",
            "22/22 [==============================] - 0s 839us/step - loss: 3.9677e-04\n",
            "Epoch 170/200\n",
            "22/22 [==============================] - 0s 840us/step - loss: 3.4471e-04\n",
            "Epoch 171/200\n",
            "22/22 [==============================] - 0s 859us/step - loss: 2.5802e-04\n",
            "Epoch 172/200\n",
            "22/22 [==============================] - 0s 853us/step - loss: 2.5761e-04\n",
            "Epoch 173/200\n",
            "22/22 [==============================] - 0s 868us/step - loss: 1.5139e-04\n",
            "Epoch 174/200\n",
            "22/22 [==============================] - 0s 832us/step - loss: 1.7473e-04\n",
            "Epoch 175/200\n",
            "22/22 [==============================] - 0s 814us/step - loss: 1.1613e-04\n",
            "Epoch 176/200\n",
            "22/22 [==============================] - 0s 843us/step - loss: 2.2281e-04\n",
            "Epoch 177/200\n",
            "22/22 [==============================] - 0s 828us/step - loss: 4.8457e-04\n",
            "Epoch 178/200\n",
            "22/22 [==============================] - 0s 827us/step - loss: 2.2415e-04\n",
            "Epoch 179/200\n",
            "22/22 [==============================] - 0s 828us/step - loss: 2.8732e-04\n",
            "Epoch 180/200\n",
            "22/22 [==============================] - 0s 817us/step - loss: 2.3132e-04\n",
            "Epoch 181/200\n",
            "22/22 [==============================] - 0s 828us/step - loss: 5.1920e-04\n",
            "Epoch 182/200\n",
            "22/22 [==============================] - 0s 829us/step - loss: 5.0991e-04\n",
            "Epoch 183/200\n",
            "22/22 [==============================] - 0s 827us/step - loss: 0.0020\n",
            "Epoch 184/200\n",
            "22/22 [==============================] - 0s 794us/step - loss: 0.0058\n",
            "Epoch 185/200\n",
            "22/22 [==============================] - 0s 804us/step - loss: 0.0049\n",
            "Epoch 186/200\n",
            "22/22 [==============================] - 0s 851us/step - loss: 0.0074\n",
            "Epoch 187/200\n",
            "22/22 [==============================] - 0s 923us/step - loss: 0.0029\n",
            "Epoch 188/200\n",
            "22/22 [==============================] - 0s 909us/step - loss: 0.0020\n",
            "Epoch 189/200\n",
            "22/22 [==============================] - 0s 877us/step - loss: 0.0029\n",
            "Epoch 190/200\n",
            "22/22 [==============================] - 0s 884us/step - loss: 0.0016\n",
            "Epoch 191/200\n",
            "22/22 [==============================] - 0s 873us/step - loss: 0.0013\n",
            "Epoch 192/200\n",
            "22/22 [==============================] - 0s 884us/step - loss: 6.4966e-04\n",
            "Epoch 193/200\n",
            "22/22 [==============================] - 0s 882us/step - loss: 5.6033e-04\n",
            "Epoch 194/200\n",
            "22/22 [==============================] - 0s 1ms/step - loss: 4.4066e-04\n",
            "Epoch 195/200\n",
            "22/22 [==============================] - 0s 934us/step - loss: 5.4835e-04\n",
            "Epoch 196/200\n",
            "22/22 [==============================] - 0s 836us/step - loss: 4.5954e-04\n",
            "Epoch 197/200\n",
            "22/22 [==============================] - 0s 853us/step - loss: 3.4000e-04\n",
            "Epoch 198/200\n",
            "22/22 [==============================] - 0s 845us/step - loss: 3.3032e-04\n",
            "Epoch 199/200\n",
            "22/22 [==============================] - 0s 843us/step - loss: 3.7122e-04\n",
            "Epoch 200/200\n",
            "22/22 [==============================] - 0s 830us/step - loss: 3.7808e-04\n",
            "Model saved as best_model.h5\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Input\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from sklearn.model_selection import train_test_split\n",
        "from hyperopt import hp, fmin, tpe, STATUS_OK, Trials\n",
        "import numpy as np\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from hyperopt import space_eval\n",
        "\n",
        "# Assuming your dataset_x_norm and dataset_y_norm are already defined and preprocessed\n",
        "\n",
        "# Define the space of hyperparameters to search\n",
        "space = {\n",
        "    'num_layers': hp.choice('num_layers', [1, 2, 3, 4,5,6,7]),\n",
        "    'units_per_layer': hp.choice('units_per_layer', [32, 64, 109, 128,140, 180,256,300]),\n",
        "    'activation': hp.choice('activation', ['relu', 'tanh', 'sigmoid']),\n",
        "    'learning_rate': hp.loguniform('learning_rate', np.log(0.0001), np.log(0.01)),\n",
        "    'epochs': hp.choice('epochs', [150, 160,170,180,185,190,195,200])\n",
        "}\n",
        "\n",
        "def objective(params):\n",
        "    model = Sequential()\n",
        "    model.add(Dense(params['units_per_layer'], input_dim=6, activation=params['activation']))\n",
        "    for _ in range(params['num_layers']):\n",
        "        model.add(Dense(params['units_per_layer'], activation=params['activation']))\n",
        "    model.add(Dense(1))  # Output layer\n",
        "    model.compile(optimizer=Adam(learning_rate=params['learning_rate']), loss='mean_squared_error')\n",
        "    \n",
        "    # Using a fixed validation split for simplicity\n",
        "    model.fit(X_train, y_train, epochs=params['epochs'], verbose=0, validation_split=0.15)\n",
        "    \n",
        "    # Evaluate the model\n",
        "    mse = model.evaluate(X_val, y_val, verbose=0)\n",
        "    return {'loss': mse, 'status': STATUS_OK}\n",
        "\n",
        "trials = Trials()\n",
        "best = fmin(objective,\n",
        "            space=space,\n",
        "            algo=tpe.suggest,\n",
        "            max_evals=10700,  # Adjusted number of evaluations\n",
        "            trials=trials)\n",
        "\n",
        "# A more efficient way to retrieve the best parameters\n",
        "best_params = space_eval(space, best)\n",
        "print(f\"Best hyperparameters: {best_params}\")\n",
        "\n",
        "# Retrain the model with the best hyperparameters\n",
        "model = Sequential()\n",
        "model.add(Dense(best_params['units_per_layer'], input_dim=6, activation=best_params['activation']))\n",
        "for _ in range(best_params['num_layers'] - 1):  # Adjust for initial layer\n",
        "    model.add(Dense(best_params['units_per_layer'], activation=best_params['activation']))\n",
        "model.add(Dense(1))\n",
        "model.compile(optimizer=Adam(learning_rate=best_params['learning_rate']), loss='mean_squared_error')\n",
        "\n",
        "# Using all data for training this time\n",
        "model.fit(np.concatenate((X_train, X_val)), np.concatenate((y_train, y_val)), epochs=best_params['epochs'], verbose=1)\n",
        "\n",
        "# Save the model\n",
        "model.save('best_model.h5')\n",
        "\n",
        "print(\"Model saved as best_model.h5\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Results\n",
        "- Plot of k-cross validation performance\n",
        "- Scatter Plot of prediction results against true values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 389,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 628
        },
        "id": "xKSkPnO4ETWD",
        "outputId": "564ee694-d414-4d21-bbd9-f838e7249dd7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'activation': 'relu', 'epochs': 200, 'learning_rate': 0.0025127745346078647, 'num_layers': 7, 'units_per_layer': 300}\n"
          ]
        }
      ],
      "source": [
        "# Loss across k folds\n",
        "print(best_params)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Loss curves"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 390,
      "metadata": {},
      "outputs": [],
      "source": [
        "def plot_loss_curve(history, epoch_size):\n",
        "    loss_train = history.history['loss']\n",
        "    loss_val = history.history['val_loss']\n",
        "    epochs = range(0,epoch_size)\n",
        "    \n",
        "    plt.plot(epochs, loss_train, 'g', label='Training loss')\n",
        "    plt.plot(epochs, loss_val, 'b', label='Validation loss')\n",
        "    \n",
        "    plt.title('Training and Validation loss')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Loss')\n",
        "    \n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "\n",
        "# # Loss across k folds\n",
        "# plot_line(arr_loss, \"Loss across k-folds\", \"Value of k\")\n",
        "\n",
        "# # Training and Validation Loss\n",
        "# plot_loss_curve(history_best_model, NUM_EPOCHS)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Prediction on Test Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 391,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "v-erJ0l_Yu4P",
        "outputId": "9cff94b2-e4ca-491b-8459-aeaa1eff7606"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "4/4 [==============================] - 0s 762us/step\n",
            "26/26 [==============================] - 0s 410us/step\n",
            "Elapsed time: 0.0829 seconds\n",
            "Maxval here is:  0.9329305\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 800x300 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import time\n",
        "\n",
        "start_time = time.time()\n",
        "Y_pred_normalized = model.predict(X_test)\n",
        "end_time = time.time()\n",
        "Y_pred_normalized_entire = model.predict(dataset_x_norm)\n",
        "# Calculate elapsed time in seconds\n",
        "elapsed_time = end_time - start_time\n",
        "print(\"Elapsed time:\", round(elapsed_time, 3), \"seconds\")\n",
        "\n",
        "\n",
        "Y_pred = scaler_output.inverse_transform(Y_pred_normalized)\n",
        "Y_pred_entire = scaler_output.inverse_transform(Y_pred_normalized_entire)\n",
        "Y_actual = scaler_output.inverse_transform(y_test)\n",
        "Y_actual_entire = np.array(df_targets)\n",
        "# Moisture Content\n",
        "scatter_plot(trueValues=Y_actual[:,0], \n",
        "             predictions=Y_pred[:,0], \n",
        "             title=\"Moisture Content\")\n",
        "a, b = np.polyfit(Y_pred[:, 0], Y_actual[:, 0], 1) # y = ax + b\n",
        "x_best_fit = np.arange(0, max(max(Y_pred[:,0]), max(Y_actual[:,0])), 1)\n",
        "plt.plot(x_best_fit, a*x_best_fit + b, c='red', label='Best fit')\n",
        "plt.legend()\n",
        "plt.savefig('../Poster/Results/obj_3_MC.svg', dpi=300,\n",
        "                bbox_inches='tight',\n",
        "                transparent=True)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Error analysis\n",
        "- R squared calculation\n",
        "- Mean accuracy error"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### R squared calculation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 392,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.9864\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import r2_score\n",
        "\n",
        "# MOISTURE CONTENT\n",
        "#   - R-squared\n",
        "# mc_r2_score = r2_score(Y_actual[:, 0], Y_pred[:, 0])\n",
        "mc_r2_score = calculate_r_squared(y_true=Y_actual[:, 0], y_pred=Y_pred[:, 0])\n",
        "print(\"{:#.4g}\".format(mc_r2_score))\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### RMSE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 393,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "RMSE_MC:  0.01063\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import mean_squared_error\n",
        "from sigfig import round\n",
        "\n",
        "#MC\n",
        "rmse_mc = np.sqrt(mean_squared_error(Y_actual[:, 0], Y_pred[:, 0]))\n",
        "print('RMSE_MC: ', \"{0:.4g}\".format(rmse_mc))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we will compare with the results from Trabelsi's paper. This is single moisture prediction \n",
        "\n",
        "R^2 : 0.993\\\n",
        "Mean Squared Error: 0.028\\\n",
        "Mean absolute Error: 0.135\\\n",
        "Min. Absolute Error: 0.004\\\n",
        "Max Absolute Error: 0.441"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 394,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "R^2: 0.9739\n",
            "Mean Squared Error:  0.0001129\n",
            "Mean Absolute Error:  0.008194\n",
            "Min Absolute Error:  0.00014189291000366033\n",
            "Max Absolute Error:  0.0395660192489623\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/zeenat/anaconda3/envs/WindowVersions/lib/python3.10/site-packages/sklearn/metrics/_regression.py:483: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import mean_squared_error, mean_absolute_error,max_error, r2_score\n",
        "from sigfig import round\n",
        "\n",
        "mc_r2_score = r2_score(y_true=Y_actual[:, 0], y_pred=Y_pred[:, 0])\n",
        "print(\"R^2: {:#.4g}\".format(mc_r2_score))\n",
        "mse_mc = mean_squared_error(Y_actual[:, 0], Y_pred[:, 0], squared=True)\n",
        "print('Mean Squared Error: ', \"{0:.4g}\".format(mse_mc))\n",
        "mae_mc = mean_absolute_error(Y_actual[:, 0], Y_pred[:, 0])\n",
        "print('Mean Absolute Error: ', \"{0:.4g}\".format(mae_mc))\n",
        "\n",
        "sums = []\n",
        "for i in range(len(Y_actual[:,0])):\n",
        "    sum = Y_actual[:,0][i] - Y_pred[:,0][i]\n",
        "    #print(Y_actual[:,0][i],\" - \",Y_pred[:,0][i],'=',sum)\n",
        "    sums.append(abs(sum))\n",
        "print(\"Min Absolute Error: \",min(sums))\n",
        "print(\"Max Absolute Error: \",max(sums))\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 395,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "R^2: 0.9837\n",
            "Mean Squared Error:  7.409e-05\n",
            "Mean Absolute Error:  0.007019\n",
            "Min Absolute Error:  3.2992744445792255e-05\n",
            "Max Absolute Error:  0.039566019248962414\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/zeenat/anaconda3/envs/WindowVersions/lib/python3.10/site-packages/sklearn/metrics/_regression.py:483: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "mc_r2_score = r2_score(Y_actual_entire[:, 0], Y_pred_entire[:, 0])\n",
        "print(\"R^2: {:#.4g}\".format(mc_r2_score))\n",
        "mse_mc = mean_squared_error(Y_actual_entire[:, 0], Y_pred_entire[:, 0], squared=True)\n",
        "print('Mean Squared Error: ', \"{0:.4g}\".format(mse_mc))\n",
        "mae_mc = mean_absolute_error(Y_actual_entire[:, 0], Y_pred_entire[:, 0])\n",
        "print('Mean Absolute Error: ', \"{0:.4g}\".format(mae_mc))\n",
        "\n",
        "sums = []\n",
        "for i in range(len(Y_actual_entire[:,0])):\n",
        "    sum = Y_actual_entire[:,0][i] - Y_pred_entire[:,0][i]\n",
        "    #print(Y_actual[:,0][i],\" - \",Y_pred[:,0][i],'=',sum)\n",
        "    sums.append(abs(sum))\n",
        "print(\"Min Absolute Error: \",min(sums))\n",
        "print(\"Max Absolute Error: \",max(sums))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
