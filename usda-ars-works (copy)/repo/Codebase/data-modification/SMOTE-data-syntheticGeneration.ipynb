{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "GRAIN_TYPE = 'Wheat'\n",
    "#GRAIN_TYPE = 'newWheatData'\n",
    "#GRAIN_TYPE = 'WheatAdded_Type'\n",
    "#GRAIN_TYPE = 'Combined_Grains'\n",
    "# GRAIN_TYPE = 'Oats'\n",
    "# GRAIN_TYPE = 'Barley'\n",
    "# GRAIN_TYPE = 'Sorghum'\n",
    "# GRAIN_TYPE = 'Soybeans'\n",
    "# GRAIN_TYPE = 'Corn'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "URL = \"../../Datasets/processed/\" + GRAIN_TYPE + \".csv\"\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv(URL)\n",
    "# Let's say 'df' is your original DataFrame\n",
    "# Filter out the samples where 'd(cm)' is 7.7\n",
    "df_77 = df[df['d(cm)'] == 7.7]\n",
    "\n",
    "# Decide how many synthetic samples you want to create\n",
    "# For example, if df_77 has 50 samples and you want to triple that amount in the dataset\n",
    "num_synthetic_samples = round(len(df_77) * 1.2)  # to add twice as many\n",
    "\n",
    "# Identify numeric columns to which you want to add noise\n",
    "numeric_columns = df_77.select_dtypes(include=[np.number]).columns.tolist()\n",
    "\n",
    "# Generate synthetic samples\n",
    "synthetic_samples = df_77.sample(n=num_synthetic_samples, replace=True)\n",
    "\n",
    "# Add some small random noise to the synthetic samples (excluding 'd(cm)' and 'Freq')\n",
    "for column in numeric_columns:\n",
    "    if column not in ['d(cm)', 'Freq']:  # Exclude 'd(cm)' and 'Freq' from noise addition\n",
    "        std_dev = df_77[column].std()\n",
    "        noise = np.random.normal(0, std_dev * 0.1, num_synthetic_samples)\n",
    "        synthetic_samples[column] += noise\n",
    "\n",
    "# Specifically handle the 'Freq' column to ensure it remains a whole number\n",
    "if 'Freq' in synthetic_samples.columns:\n",
    "    std_dev_freq = df_77['Freq'].std()\n",
    "    noise_freq = np.random.normal(0, std_dev_freq * 0.1, num_synthetic_samples)\n",
    "    synthetic_samples['Freq'] = (synthetic_samples['Freq'] + noise_freq).round().astype(int)\n",
    "\n",
    "# Ensure 'Freq' does not go below a certain threshold, if applicable\n",
    "# synthetic_samples['Freq'] = synthetic_samples['Freq'].clip(lower=minimum_valid_freq_value)\n",
    "\n",
    "# Concatenate the synthetic samples back to the original dataset\n",
    "df_augmented = pd.concat([df, synthetic_samples])\n",
    "\n",
    "# Make sure to reset the index as concatenation can result in duplicate indices\n",
    "df_augmented.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Specify the path for your new file\n",
    "output_file_path = '../../Datasets/processed/augmented_' + GRAIN_TYPE + '.csv'  # Update this to your desired file path\n",
    "\n",
    "# Save the augmented DataFrame to a new CSV file\n",
    "df_augmented.to_csv(output_file_path, index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'GRAIN_TYPE' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1779760/668061541.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mURL\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"../../Datasets/processed/\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mGRAIN_TYPE\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\".csv\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# Load the dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'GRAIN_TYPE' is not defined"
     ]
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "URL = \"../../Datasets/processed/\" + GRAIN_TYPE + \".csv\"\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv(URL)\n",
    "# Calculate correlation matrix\n",
    "df_cleaned = df.drop(columns=['Unnamed: 0', 'Density'], errors='ignore')\n",
    "\n",
    "# Now let's recalculate the correlation matrix without these columns\n",
    "corr_cleaned = df_cleaned.corr()\n",
    "\n",
    "# Create a mask to hide the upper triangle of the correlation matrix\n",
    "mask = np.triu(np.ones_like(corr_cleaned, dtype=bool))\n",
    "mask = mask[1:, :-1]\n",
    "corr = corr_cleaned.iloc[1:,:-1]\n",
    "# Generate a heatmap for the cleaned correlation matrix\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(corr_cleaned, annot=True, fmt=\".2f\", cmap='coolwarm', cbar=True, square=True)\n",
    "plt.title('Correlation Matrix Heatmap (Lower Triangle, Cleaned)')\n",
    "plt.show()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
