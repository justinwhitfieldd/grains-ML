Codebase/1-1_dnn_kcv_normalize_after_split_usda_data.ipynb
	- file where the R2 and rmse values for corn were generated for the presentation
	
Codebase/2_dnn_separate_output_kcv_usda_data .ipynb
	- very cool graph of model generated here
		
 	
add new metric 
standard error performance
samir trabelsi




==========================================================
(Entire wheat dataset, single target)
epoch:190
LR: 0.00091
model: layers.Dense(109, input_shape=(6,), activation='relu',),
    layers.Dense(109, activation='relu', ),
    layers.Dense(109, activation='relu',),
    layers.Dense(1, activation='linear')
R^2: 0.9993
Mean Squared Error:  0.008625
Mean Absolute Error:  0.06327
Min Absolute Error:  0.0005446624755869323
Max Absolute Error:  0.40733703613281236

learning_rate=0.00091
NUM_EPOCHS = 190
    layers.Dense(128, input_shape=(6,), activation='relu',),
    layers.Dense(128, activation='relu', ),
    layers.Dense(128, activation='relu',),
    layers.Dense(1, activation='linear')  # Output layer with 2 neurons for the two regression targets
  ])
R^2: 0.9994
Mean Squared Error:  0.007797
Mean Absolute Error:  0.0615
Min Absolute Error:  0.0003349685668947444
Max Absolute Error:  0.3897646331787108
====================================

train model for bulk density, 
      train for single output(bd) and for two output (m% and bd)
      Look into training classification model for grain type and variety
              
              Bulk density
      R^2: 0.9739
      Mean Squared Error:  0.0001129
      Mean Absolute Error:  0.008194
      Min Absolute Error:  0.00014189291000366033
      Max Absolute Error:  0.0395660192489623
      Best hyperparameters: {'activation': 'relu', 'epochs': 200, 'learning_rate': 0.0025127745346078647, 'num_layers': 7, 'units_per_layer': 300}


      R^2: 0.9709
      Mean Squared Error:  0.000126
      Mean Absolute Error:  0.009132
      Min Absolute Error:  0.0003164817810059617
      Max Absolute Error:  0.02897950019836415
      Best hyperparameters: {'activation': 'relu', 'epochs': 195, 'learning_rate': 0.002898773627262781, 'num_layers': 7, 'units_per_layer': 400}

      R^2: 0.9939
      Mean Squared Error:  2.77e-05
      Mean Absolute Error:  0.003728
      Min Absolute Error:  1.0248184204586508e-06
      Max Absolute Error:  0.027908093261718725
      layers.Dense(450, input_shape=(6,), activation='relu',),
          layers.Dense(450, activation='relu', ),
          layers.Dense(450, activation='relu',),
          layers.Dense(450, activation='relu',),
          layers.Dense(450, activation='relu',),
          layers.Dense(450, activation='relu',),
          layers.Dense(1, activation='linear') 
        ])
        learning_rate=0.002898773627262781
        NUM_EPOCHS = 220

      relu on hidden layers, sigmoid on output layer
      {'batch_size': 8, 'epochs': 210, 'learning_rate': 0.0011007913080333014, 'num_layers': 5, 'units_per_layer': 180}
      R^2: 0.9966
      Mean Squared Error:  1.623e-05
      Mean Absolute Error:  0.002426
      Min Absolute Error:  1.208839416511065e-05
      Max Absolute Error:  0.019702413749694725


      Best hyperparameters: {'batch_size': 10, 'epochs': 225, 'learning_rate': 0.0005541384023326294, 'num_layers': 6, 'units_per_layer': 450}
      R^2: 0.9967
      Mean Squared Error:  1.579e-05
      Mean Absolute Error:  0.002604
      Min Absolute Error:  3.01952362058433e-06
      Max Absolute Error:  0.015751460266113182


  classification
      ======================================================================================
      Loss: 0.8480
      Accuracy: 0.7851
      Precision Class 0: 0.7200
      Recall Class 0: 0.6429
      Precision Class 1: 0.8571
      Recall Class 1: 0.8276
      Precision Class 2: 0.8400
      Recall Class 2: 0.7778
      Precision Class 3: 0.8462
      Recall Class 3: 0.9565
      Precision Class 4: 0.5882
      Recall Class 4: 0.7143

      Accuracy: 0.8778
      Precision Class 0: 0.8750
      Recall Class 0: 0.9130
      Precision Class 1: 0.7143
      Recall Class 1: 0.8333
      Precision Class 2: 0.8571
      Recall Class 2: 0.6667
      Precision Class 3: 1.0000
      Recall Class 3: 1.0000
      Precision Class 4: 0.9048
      Recall Class 4: 0.9048
      epochs=400, batch_size=32 learning_rate=0.0003

      Accuracy: 0.8889
      Precision Class 0: 0.9130
      Recall Class 0: 0.9130
      Precision Class 1: 0.8333
      Recall Class 1: 0.8333
      Precision Class 2: 0.8235
      Recall Class 2: 0.7778
      Precision Class 3: 1.0000
      Recall Class 3: 1.0000
      Precision Class 4: 0.8571
      Recall Class 4: 0.8571
      epochs=800, batch_size=32, learning_rate=0.0002

      Accuracy: 0.9167
      Precision Class 0: 0.8400
      Recall Class 0: 0.8750
      Precision Class 1: 1.0000
      Recall Class 1: 0.8333
      Precision Class 2: 0.8846
      Recall Class 2: 0.9583
      Precision Class 3: 1.0000
      Recall Class 3: 1.0000
      Precision Class 4: 0.9167
      Recall Class 4: 0.9167
      epochs=900, batch_size=32)learning_rate=0.0002

      here i switched from 128,256,256,5 to 256,256,256,5
      Accuracy: 0.9250
      Precision Class 0: 0.8400
      Recall Class 0: 0.8750
      Precision Class 1: 1.0000
      Recall Class 1: 0.8333
      Precision Class 2: 0.8846
      Recall Class 2: 0.9583
      Precision Class 3: 1.0000
      Recall Class 3: 1.0000
      Precision Class 4: 0.9200
      Recall Class 4: 0.9583
      learning_rate=0.0003 epochs=1000, batch_size=8

      Loss: 0.3000
      Accuracy: 0.9417
      Precision Class 0: 0.9130
      Recall Class 0: 0.8750
      Precision Class 1: 1.0000
      Recall Class 1: 0.9167
      Precision Class 2: 0.8846
      Recall Class 2: 0.9583
      Precision Class 3: 1.0000
      Recall Class 3: 1.0000
      Precision Class 4: 0.9200
      Recall Class 4: 0.9583
      epochs=1100, batch_size=8 learning_rate=0.0003

      Loss: 0.3331
      Accuracy: 0.9500
      Precision Class 0: 0.8750
      Recall Class 0: 0.8750
      Precision Class 1: 1.0000
      Recall Class 1: 0.9583
      Precision Class 2: 0.9565
      Recall Class 2: 0.9167
      Precision Class 3: 1.0000
      Recall Class 3: 1.0000
      Precision Class 4: 0.9231
      Recall Class 4: 1.0000
      epochs=1100, batch_size=8) earning_rate=0.00034

      Loss: 0.2541
      Accuracy: 0.9583
      Precision Class 0: 0.9545
      Recall Class 0: 0.8750
      Precision Class 1: 1.0000
      Recall Class 1: 1.0000
      Precision Class 2: 0.9200
      Recall Class 2: 0.9583
      Precision Class 3: 1.0000
      Recall Class 3: 1.0000
      Precision Class 4: 0.9200
      Recall Class 4: 0.9583
      learning_rate=0.00033 epochs=1100, batch_size=8

      =================================================================
      Class  0 | Precision: 0.955 | Recall: 0.875 | Accuracy: 0.875
      Class  1 | Precision: 1.000 | Recall: 1.000 | Accuracy: 1.000
      Class  2 | Precision: 0.920 | Recall: 0.958 | Accuracy: 0.958
      Class  3 | Precision: 1.000 | Recall: 1.000 | Accuracy: 1.000
      Class  4 | Precision: 0.920 | Recall: 0.958 | Accuracy: 0.958

      Avg Precision: 0.959 | Avg Recall: 0.958 | Avg Accuracy: 0.958
      HYPERPARAMS: LR: 0.00033  EPOCH: 1100  BATCH: 8

      even dataset split between classes
=================================================================


Bulk density AND moisture content
 my_model = Sequential([
    
    layers.Dense(450, input_shape=(6,), activation='relu',),
    layers.Dense(450, activation='relu', ),
    layers.Dense(450, activation='relu',),
    layers.Dense(450, activation='relu',),
    layers.Dense(450, activation='relu',),
    layers.Dense(450, activation='relu',),
    layers.Dense(2, activation='linear')  # Output layer with 2 neurons for the two regression targets
  ])

  opt = tf.keras.optimizers.Adam(learning_rate=0.002898773627262781) # 0.0006 
  NUM_EPOCHS = 300# 180
BATCH_SIZE = 10
K_FOLD_SPLITS = 10
MC
R^2: 0.9955
Mean Squared Error:  0.06389
Mean Absolute Error:  0.1811
Min Absolute Error:  0.00021045684814424703
Max Absolute Error:  0.8301896286010741
BD
R^2: 0.9365
Mean Squared Error:  0.0003015
Mean Absolute Error:  0.01315
Min Absolute Error:  0.00018147468566898084
Max Absolute Error:  0.04639030332565308